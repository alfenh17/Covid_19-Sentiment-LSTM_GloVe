{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5IoMMVfkmy7"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wD6eYOozhekF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "import nltk\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer                           \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import random\n",
        "import string,re\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words(\"english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrB3H3Sh8RXK",
        "outputId": "38da79f6-8250-4c3b-8b9c-e41f9edc5a1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxhB1kQNmLqy"
      },
      "source": [
        "# Cek Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-8_dbyp_mNnB"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data dari Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkvet5mgb_40",
        "outputId": "fc5e2e21-0bb9-43a8-910b-1cf21c6a53d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Data"
      ],
      "metadata": {
        "id": "VVxPHzFSROM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = torch.load('/content/gdrive/My Drive/Skripsi/P1/train_loader.pt',)\n",
        "validation_data = torch.load('/content/gdrive/My Drive/Skripsi/P1/valid_loader.pt')\n",
        "test_data = torch.load('/content/gdrive/My Drive/Skripsi/P1/test_loader.pt')"
      ],
      "metadata": {
        "id": "1BAi8kJHRRXe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_data.dataset.tensors[0].size())\n",
        "display(validation_data.dataset.tensors[0].size())\n",
        "display(test_data.dataset.tensors[0].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "TEZm4rVeSopk",
        "outputId": "c3a8d9ed-8ba8-459e-aee1-3dffc3e1af0f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([489, 32])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([389, 32])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([49, 32])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = []\n",
        "label_train = []\n",
        "for input,target in zip(train_data.dataset.tensors[0],train_data.dataset.tensors[1]):\n",
        "  data_train.append(input)\n",
        "  label_train.append(target)\n",
        "data_train_2 = []\n",
        "label_train_2 = []\n",
        "for data in data_train:\n",
        "  data_train_2.append(data.tolist())\n",
        "for lab in label_train:\n",
        "  label_train_2.append(lab.tolist())"
      ],
      "metadata": {
        "id": "3tu0X3PHB-hy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_val = []\n",
        "label_val = []\n",
        "for input,target in zip(validation_data.dataset.tensors[0],validation_data.dataset.tensors[1]):\n",
        "  data_val.append(input)\n",
        "  label_val.append(target)\n",
        "data_val_2 = []\n",
        "label_val_2 = []\n",
        "for data in data_val:\n",
        "  data_val_2.append(data.tolist())\n",
        "for lab in label_val:\n",
        "  label_val_2.append(lab.tolist())"
      ],
      "metadata": {
        "id": "wtLy-57gEIVS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = []\n",
        "label_test = []\n",
        "for input,target in zip(test_data.dataset.tensors[0],test_data.dataset.tensors[1]):\n",
        "  data_test.append(input)\n",
        "  label_test.append(target)\n",
        "data_test_2 = []\n",
        "label_test_2 = []\n",
        "for data in data_test:\n",
        "  data_test_2.append(data.tolist())\n",
        "for lab in label_test:\n",
        "  label_test_2.append(lab.tolist())"
      ],
      "metadata": {
        "id": "wuCkjdcwPx_6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = torch.stack(data_train)\n",
        "data_val = torch.stack(data_val)\n",
        "data_test= torch.stack(data_test)"
      ],
      "metadata": {
        "id": "islGUP4yRkbW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_train = torch.stack(label_train)\n",
        "label_val = torch.stack(label_val)\n",
        "label_test = torch.stack(label_test)"
      ],
      "metadata": {
        "id": "ZC7B9bmTSzBX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(data_train, label_train)\n",
        "valid_data = TensorDataset(data_val, label_val)\n",
        "test_data = TensorDataset(data_test, label_test)"
      ],
      "metadata": {
        "id": "aLeuNRw1QYAf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 64\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "# Masukkan data loader ke GPU\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size = batch, num_workers=0)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size = batch, num_workers=0)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size = batch, num_workers=0)"
      ],
      "metadata": {
        "id": "aGhYUupEUZ4d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model"
      ],
      "metadata": {
        "id": "kvbel2rmQj4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = torch.load('/content/gdrive/My Drive/Skripsi/Finale/best_model_sentiment_nets_82acc.pt')"
      ],
      "metadata": {
        "id": "EUtBPndjQkqK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEfdALvVMBiL"
      },
      "source": [
        "##Import Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweet = pd.read_excel('/content/gdrive/My Drive/Skripsi/P1/P1_185150207111021_Alfen Hasiholan/tweet_1000.xlsx')[['tweet','sentiment']]"
      ],
      "metadata": {
        "id": "WztC7yQJbISz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "f40QoKwXdzBa",
        "outputId": "c5834fcc-d967-4c26-ccf4-5be3127a31c6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 tweet  sentiment\n",
              "0    @mdevitoboutin @GulfFLLinda I hope you get bet...        0.0\n",
              "1    #COVID19\\n#COVID-19\\n#COVID\\n#Omicron \\nThe CO...        0.0\n",
              "2    @abcnews Because people are not falling for th...        1.0\n",
              "3    wvstatejournal: RT @WVNews247: ACTIVE CASE COU...        1.0\n",
              "4    @Unusual_Times I think there is a far bit of e...        1.0\n",
              "..                                                 ...        ...\n",
              "995  If omicron is doing a better job than vaccines...        0.0\n",
              "996  I'm not taking my mask off yet.\\n\\nCoronavirus...        1.0\n",
              "997  It’s irrelevant whether you (falsely) believe ...        0.0\n",
              "998  @lwtnikki Indeed they are! In Malaysia also we...        1.0\n",
              "999  @garthmullins @JeanGuy_LeB Family, friends, co...        1.0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7fc456a-1450-4e74-af3a-bfc244cb0bab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@mdevitoboutin @GulfFLLinda I hope you get bet...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#COVID19\\n#COVID-19\\n#COVID\\n#Omicron \\nThe CO...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@abcnews Because people are not falling for th...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wvstatejournal: RT @WVNews247: ACTIVE CASE COU...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Unusual_Times I think there is a far bit of e...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>If omicron is doing a better job than vaccines...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>I'm not taking my mask off yet.\\n\\nCoronavirus...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>It’s irrelevant whether you (falsely) believe ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>@lwtnikki Indeed they are! In Malaysia also we...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>@garthmullins @JeanGuy_LeB Family, friends, co...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7fc456a-1450-4e74-af3a-bfc244cb0bab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7fc456a-1450-4e74-af3a-bfc244cb0bab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7fc456a-1450-4e74-af3a-bfc244cb0bab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FiSdYzIq8rYH"
      },
      "outputs": [],
      "source": [
        "tokenized_tweet = pickle.load(open('/content/gdrive/My Drive/Skripsi/P1/tweet_1000_gloved.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_tweet = pickle.load(open('/content/gdrive/My Drive/Skripsi/P1/P1_185150207111021_Alfen Hasiholan/tokenized_tweets_paded_1000.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "mMW9UfGdvpkQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import GloVe Pre-trained"
      ],
      "metadata": {
        "id": "BbAZgG3HMjll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sT5Au2_1_goD"
      },
      "outputs": [],
      "source": [
        "# import model glove dari library torchtext\n",
        "glove_model = pickle.load(open('/content/gdrive/My Drive/Skripsi/P1/P1_185150207111021_Alfen Hasiholan/glove.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chSUnNml-Ycr"
      },
      "source": [
        "# Delete Netral Tweets\n",
        "\n",
        "Delete tweets that have netral labels. (-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WFEocKVBl4KP"
      },
      "outputs": [],
      "source": [
        "df_tweet = df_tweet[df_tweet.sentiment != '-']\n",
        "df_tweet.loc[:,'tweet'] = df_tweet['tweet'].astype('str')\n",
        "df_tweet.loc[:,'sentiment'] = df_tweet['sentiment'].astype('int64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CogzTiycLlb"
      },
      "source": [
        "# Preprocessing Tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, create a slang word dictionary to convert slang words to its own origin words."
      ],
      "metadata": {
        "id": "ah3ZMUf58F4C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "amarY6pkBqgS"
      },
      "outputs": [],
      "source": [
        "# Source : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
        "slangs = {'afaik':'as far as i know','afk':' away from keyboard', 'asap': 'as soon as possible', 'atk': 'at the keyboard',\n",
        "          'atm': 'at the moment', 'a3': 'anytime, anywhere, anyplace', 'bak': 'back at keyboard', 'bbl': 'be back later',\n",
        "          'bbs': 'be back soon', 'bfn': 'bye for now', 'b4n': 'bye for now', 'brb' : 'be right back', 'brt': 'be right there',\n",
        "          'btw': 'by the way', 'b4': 'before', 'b4n': 'bye for now', 'cu': 'see you', 'cul8r': 'see you later', 'cya': 'see you',\n",
        "          'faq': 'frequently asked questions', 'fc': 'fingers crossed', 'fwiw': \"for what it's worth\", 'fyi': 'for your information',\n",
        "          'gal': 'get a life', 'gg': 'good game', 'gn': 'good night', 'gmta': 'great minds think alike', 'gr8': 'great!', 'g9': 'genius',\n",
        "          'ic': 'i see', 'icq': 'i seek you', 'ilu': 'i love you', 'imho': 'in my honest/humble opinion', 'imo': 'in my opinion', \n",
        "          'iow': 'in other words', 'irl': 'in real life', 'kiss': 'keep it simple, stupid', 'ldr': 'long distance relationship', \n",
        "          'lmao': 'laughing my ass off', 'lol': 'laughing out loud', 'ltns': 'long time no see', 'luv': 'love', 'l8r': 'later', 'mte': 'my thoughts exactly',\n",
        "          'm8': 'mate', 'nrn': 'no reply necessary', 'oic': 'oh i see', 'pita': 'pain in the ass', 'prt': 'party', 'prw': 'parents are watching',\n",
        "          'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud', 'rotflmao': 'rolling on the floor laughing my ass off',\n",
        "          'sk8': 'skate', 'asl': 'age, sex, location', 'thx': 'thank you', 'ttfn': 'ta-ta for now!', 'ttyl': 'talk to you later', \n",
        "          'u': 'you', 'u2': 'you too', 'u4e': 'yours for ever', 'wb': 'welcome back', 'wtf': 'what the fuck', 'wtg': 'way to go!', \n",
        "          'wuf': 'where are you from?', 'w8': 'wait'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YKGaHb4xE2d9"
      },
      "outputs": [],
      "source": [
        "# Sumber : https://gist.github.com/sebleier/554280\n",
        "stopwords_2 = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \n",
        "              \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \n",
        "              \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \n",
        "              \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \n",
        "              \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\",\n",
        "              \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\",\n",
        "              \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \n",
        "              \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \n",
        "              \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \n",
        "              \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \n",
        "              \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \n",
        "              \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \n",
        "              \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \n",
        "              \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \n",
        "              \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \n",
        "              \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \n",
        "              \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \n",
        "              \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \n",
        "              \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\",\n",
        "              \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\",\n",
        "              \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\",\n",
        "              \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \n",
        "              \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \n",
        "              \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \n",
        "              \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \n",
        "              \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \n",
        "              \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\",\n",
        "              \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \n",
        "              \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \n",
        "              \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \n",
        "              \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \n",
        "              \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \n",
        "              \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \n",
        "              \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\",\n",
        "              \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \n",
        "              \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \n",
        "              \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \n",
        "              \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \n",
        "              \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \n",
        "              \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \n",
        "              \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \n",
        "              \"ng\", \"ni\", \"nine\", \"nisentiment_netsy\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nosentiment_netsheless\", \"noone\", \"nor\", \"normally\", \"nos\",\n",
        "              \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \n",
        "              \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \n",
        "              \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \n",
        "              \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \n",
        "              \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \n",
        "              \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \n",
        "              \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \n",
        "              \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \n",
        "              \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \n",
        "              \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\",\n",
        "              \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \n",
        "              \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \n",
        "              \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \n",
        "              \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \n",
        "              \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \n",
        "              \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \n",
        "              \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \n",
        "              \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \n",
        "              \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \n",
        "              \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \n",
        "              \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \n",
        "              \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \n",
        "              \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \n",
        "              \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \n",
        "              \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \n",
        "              \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \n",
        "              \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \n",
        "              \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \n",
        "              \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \n",
        "              \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \n",
        "              \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \n",
        "              \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \n",
        "              \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \n",
        "              \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \n",
        "              \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \n",
        "              \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \n",
        "              \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the ' character into ’."
      ],
      "metadata": {
        "id": "AZL68AOI8k_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "W-AaOieBHWrN"
      },
      "outputs": [],
      "source": [
        "def fix_stopword(sw):\n",
        "    stopwordss = []\n",
        "    for stopword in stopwords_2:\n",
        "        if \"'\" in stopword:\n",
        "            stopword = stopword.replace(\"'\",\"’\")\n",
        "            stopwordss.append(stopword)\n",
        "        else :\n",
        "            stopwordss.append(stopword)\n",
        "    return stopwordss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzbBXRmAMtOG",
        "outputId": "ba62ad96-057e-49f7-fdb4-e318904e526e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0o',\n",
              " '0s',\n",
              " '3a',\n",
              " '3b',\n",
              " '3d',\n",
              " '6b',\n",
              " '6o',\n",
              " 'a',\n",
              " 'a1',\n",
              " 'a2',\n",
              " 'a3',\n",
              " 'a4',\n",
              " 'ab',\n",
              " 'able',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abst',\n",
              " 'ac',\n",
              " 'accordance',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'across',\n",
              " 'act',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'added',\n",
              " 'adj',\n",
              " 'ae',\n",
              " 'af',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affects',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'ag',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ah',\n",
              " 'ain',\n",
              " 'ain’t',\n",
              " 'aj',\n",
              " 'al',\n",
              " 'all',\n",
              " 'allow',\n",
              " 'allows',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amoungst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'announce',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anybody',\n",
              " 'anyhow',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anyways',\n",
              " 'anywhere',\n",
              " 'ao',\n",
              " 'ap',\n",
              " 'apart',\n",
              " 'apparently',\n",
              " 'appear',\n",
              " 'appreciate',\n",
              " 'appropriate',\n",
              " 'approximately',\n",
              " 'ar',\n",
              " 'are',\n",
              " 'aren',\n",
              " 'arent',\n",
              " 'aren’t',\n",
              " 'arise',\n",
              " 'around',\n",
              " 'as',\n",
              " 'a’s',\n",
              " 'aside',\n",
              " 'ask',\n",
              " 'asking',\n",
              " 'associated',\n",
              " 'at',\n",
              " 'au',\n",
              " 'auth',\n",
              " 'av',\n",
              " 'available',\n",
              " 'aw',\n",
              " 'away',\n",
              " 'awfully',\n",
              " 'ax',\n",
              " 'ay',\n",
              " 'az',\n",
              " 'b',\n",
              " 'b1',\n",
              " 'b2',\n",
              " 'b3',\n",
              " 'ba',\n",
              " 'back',\n",
              " 'bc',\n",
              " 'bd',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'begin',\n",
              " 'beginning',\n",
              " 'beginnings',\n",
              " 'begins',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'believe',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'best',\n",
              " 'better',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'bi',\n",
              " 'bill',\n",
              " 'biol',\n",
              " 'bj',\n",
              " 'bk',\n",
              " 'bl',\n",
              " 'bn',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'bp',\n",
              " 'br',\n",
              " 'brief',\n",
              " 'briefly',\n",
              " 'bs',\n",
              " 'bt',\n",
              " 'bu',\n",
              " 'but',\n",
              " 'bx',\n",
              " 'by',\n",
              " 'c',\n",
              " 'c1',\n",
              " 'c2',\n",
              " 'c3',\n",
              " 'ca',\n",
              " 'call',\n",
              " 'came',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'cant',\n",
              " 'can’t',\n",
              " 'cause',\n",
              " 'causes',\n",
              " 'cc',\n",
              " 'cd',\n",
              " 'ce',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'cf',\n",
              " 'cg',\n",
              " 'ch',\n",
              " 'changes',\n",
              " 'ci',\n",
              " 'cit',\n",
              " 'cj',\n",
              " 'cl',\n",
              " 'clearly',\n",
              " 'cm',\n",
              " 'c’mon',\n",
              " 'cn',\n",
              " 'co',\n",
              " 'com',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'con',\n",
              " 'concerning',\n",
              " 'consequently',\n",
              " 'consider',\n",
              " 'considering',\n",
              " 'contain',\n",
              " 'containing',\n",
              " 'contains',\n",
              " 'corresponding',\n",
              " 'could',\n",
              " 'couldn',\n",
              " 'couldnt',\n",
              " 'couldn’t',\n",
              " 'course',\n",
              " 'cp',\n",
              " 'cq',\n",
              " 'cr',\n",
              " 'cry',\n",
              " 'cs',\n",
              " 'c’s',\n",
              " 'ct',\n",
              " 'cu',\n",
              " 'currently',\n",
              " 'cv',\n",
              " 'cx',\n",
              " 'cy',\n",
              " 'cz',\n",
              " 'd',\n",
              " 'd2',\n",
              " 'da',\n",
              " 'date',\n",
              " 'dc',\n",
              " 'dd',\n",
              " 'de',\n",
              " 'definitely',\n",
              " 'describe',\n",
              " 'described',\n",
              " 'despite',\n",
              " 'detail',\n",
              " 'df',\n",
              " 'di',\n",
              " 'did',\n",
              " 'didn',\n",
              " 'didn’t',\n",
              " 'different',\n",
              " 'dj',\n",
              " 'dk',\n",
              " 'dl',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " 'doesn’t',\n",
              " 'doing',\n",
              " 'don',\n",
              " 'done',\n",
              " 'don’t',\n",
              " 'down',\n",
              " 'downwards',\n",
              " 'dp',\n",
              " 'dr',\n",
              " 'ds',\n",
              " 'dt',\n",
              " 'du',\n",
              " 'due',\n",
              " 'during',\n",
              " 'dx',\n",
              " 'dy',\n",
              " 'e',\n",
              " 'e2',\n",
              " 'e3',\n",
              " 'ea',\n",
              " 'each',\n",
              " 'ec',\n",
              " 'ed',\n",
              " 'edu',\n",
              " 'ee',\n",
              " 'ef',\n",
              " 'effect',\n",
              " 'eg',\n",
              " 'ei',\n",
              " 'eight',\n",
              " 'eighty',\n",
              " 'either',\n",
              " 'ej',\n",
              " 'el',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'em',\n",
              " 'empty',\n",
              " 'en',\n",
              " 'end',\n",
              " 'ending',\n",
              " 'enough',\n",
              " 'entirely',\n",
              " 'eo',\n",
              " 'ep',\n",
              " 'eq',\n",
              " 'er',\n",
              " 'es',\n",
              " 'especially',\n",
              " 'est',\n",
              " 'et',\n",
              " 'et-al',\n",
              " 'etc',\n",
              " 'eu',\n",
              " 'ev',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everybody',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'ex',\n",
              " 'exactly',\n",
              " 'example',\n",
              " 'except',\n",
              " 'ey',\n",
              " 'f',\n",
              " 'f2',\n",
              " 'fa',\n",
              " 'far',\n",
              " 'fc',\n",
              " 'few',\n",
              " 'ff',\n",
              " 'fi',\n",
              " 'fifteen',\n",
              " 'fifth',\n",
              " 'fify',\n",
              " 'fill',\n",
              " 'find',\n",
              " 'fire',\n",
              " 'first',\n",
              " 'five',\n",
              " 'fix',\n",
              " 'fj',\n",
              " 'fl',\n",
              " 'fn',\n",
              " 'fo',\n",
              " 'followed',\n",
              " 'following',\n",
              " 'follows',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forth',\n",
              " 'forty',\n",
              " 'found',\n",
              " 'four',\n",
              " 'fr',\n",
              " 'from',\n",
              " 'front',\n",
              " 'fs',\n",
              " 'ft',\n",
              " 'fu',\n",
              " 'full',\n",
              " 'further',\n",
              " 'furthermore',\n",
              " 'fy',\n",
              " 'g',\n",
              " 'ga',\n",
              " 'gave',\n",
              " 'ge',\n",
              " 'get',\n",
              " 'gets',\n",
              " 'getting',\n",
              " 'gi',\n",
              " 'give',\n",
              " 'given',\n",
              " 'gives',\n",
              " 'giving',\n",
              " 'gj',\n",
              " 'gl',\n",
              " 'go',\n",
              " 'goes',\n",
              " 'going',\n",
              " 'gone',\n",
              " 'got',\n",
              " 'gotten',\n",
              " 'gr',\n",
              " 'greetings',\n",
              " 'gs',\n",
              " 'gy',\n",
              " 'h',\n",
              " 'h2',\n",
              " 'h3',\n",
              " 'had',\n",
              " 'hadn',\n",
              " 'hadn’t',\n",
              " 'happens',\n",
              " 'hardly',\n",
              " 'has',\n",
              " 'hasn',\n",
              " 'hasnt',\n",
              " 'hasn’t',\n",
              " 'have',\n",
              " 'haven',\n",
              " 'haven’t',\n",
              " 'having',\n",
              " 'he',\n",
              " 'hed',\n",
              " 'he’d',\n",
              " 'he’ll',\n",
              " 'hello',\n",
              " 'help',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'heres',\n",
              " 'here’s',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'hes',\n",
              " 'he’s',\n",
              " 'hh',\n",
              " 'hi',\n",
              " 'hid',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'hither',\n",
              " 'hj',\n",
              " 'ho',\n",
              " 'home',\n",
              " 'hopefully',\n",
              " 'how',\n",
              " 'howbeit',\n",
              " 'however',\n",
              " 'how’s',\n",
              " 'hr',\n",
              " 'hs',\n",
              " 'http',\n",
              " 'hu',\n",
              " 'hundred',\n",
              " 'hy',\n",
              " 'i',\n",
              " 'i2',\n",
              " 'i3',\n",
              " 'i4',\n",
              " 'i6',\n",
              " 'i7',\n",
              " 'i8',\n",
              " 'ia',\n",
              " 'ib',\n",
              " 'ibid',\n",
              " 'ic',\n",
              " 'id',\n",
              " 'i’d',\n",
              " 'ie',\n",
              " 'if',\n",
              " 'ig',\n",
              " 'ignored',\n",
              " 'ih',\n",
              " 'ii',\n",
              " 'ij',\n",
              " 'il',\n",
              " 'i’ll',\n",
              " 'im',\n",
              " 'i’m',\n",
              " 'immediate',\n",
              " 'immediately',\n",
              " 'importance',\n",
              " 'important',\n",
              " 'in',\n",
              " 'inasmuch',\n",
              " 'inc',\n",
              " 'indeed',\n",
              " 'index',\n",
              " 'indicate',\n",
              " 'indicated',\n",
              " 'indicates',\n",
              " 'information',\n",
              " 'inner',\n",
              " 'insofar',\n",
              " 'instead',\n",
              " 'interest',\n",
              " 'into',\n",
              " 'invention',\n",
              " 'inward',\n",
              " 'io',\n",
              " 'ip',\n",
              " 'iq',\n",
              " 'ir',\n",
              " 'is',\n",
              " 'isn',\n",
              " 'isn’t',\n",
              " 'it',\n",
              " 'itd',\n",
              " 'it’d',\n",
              " 'it’ll',\n",
              " 'its',\n",
              " 'it’s',\n",
              " 'itself',\n",
              " 'iv',\n",
              " 'i’ve',\n",
              " 'ix',\n",
              " 'iy',\n",
              " 'iz',\n",
              " 'j',\n",
              " 'jj',\n",
              " 'jr',\n",
              " 'js',\n",
              " 'jt',\n",
              " 'ju',\n",
              " 'just',\n",
              " 'k',\n",
              " 'ke',\n",
              " 'keep',\n",
              " 'keeps',\n",
              " 'kept',\n",
              " 'kg',\n",
              " 'kj',\n",
              " 'km',\n",
              " 'know',\n",
              " 'known',\n",
              " 'knows',\n",
              " 'ko',\n",
              " 'l',\n",
              " 'l2',\n",
              " 'la',\n",
              " 'largely',\n",
              " 'last',\n",
              " 'lately',\n",
              " 'later',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'lb',\n",
              " 'lc',\n",
              " 'le',\n",
              " 'least',\n",
              " 'les',\n",
              " 'less',\n",
              " 'lest',\n",
              " 'let',\n",
              " 'lets',\n",
              " 'let’s',\n",
              " 'lf',\n",
              " 'like',\n",
              " 'liked',\n",
              " 'likely',\n",
              " 'line',\n",
              " 'little',\n",
              " 'lj',\n",
              " 'll',\n",
              " 'll',\n",
              " 'ln',\n",
              " 'lo',\n",
              " 'look',\n",
              " 'looking',\n",
              " 'looks',\n",
              " 'los',\n",
              " 'lr',\n",
              " 'ls',\n",
              " 'lt',\n",
              " 'ltd',\n",
              " 'm',\n",
              " 'm2',\n",
              " 'ma',\n",
              " 'made',\n",
              " 'mainly',\n",
              " 'make',\n",
              " 'makes',\n",
              " 'many',\n",
              " 'may',\n",
              " 'maybe',\n",
              " 'me',\n",
              " 'mean',\n",
              " 'means',\n",
              " 'meantime',\n",
              " 'meanwhile',\n",
              " 'merely',\n",
              " 'mg',\n",
              " 'might',\n",
              " 'mightn',\n",
              " 'mightn’t',\n",
              " 'mill',\n",
              " 'million',\n",
              " 'mine',\n",
              " 'miss',\n",
              " 'ml',\n",
              " 'mn',\n",
              " 'mo',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'mr',\n",
              " 'mrs',\n",
              " 'ms',\n",
              " 'mt',\n",
              " 'mu',\n",
              " 'much',\n",
              " 'mug',\n",
              " 'must',\n",
              " 'mustn',\n",
              " 'mustn’t',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'n',\n",
              " 'n2',\n",
              " 'na',\n",
              " 'name',\n",
              " 'namely',\n",
              " 'nay',\n",
              " 'nc',\n",
              " 'nd',\n",
              " 'ne',\n",
              " 'near',\n",
              " 'nearly',\n",
              " 'necessarily',\n",
              " 'necessary',\n",
              " 'need',\n",
              " 'needn',\n",
              " 'needn’t',\n",
              " 'needs',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'new',\n",
              " 'next',\n",
              " 'ng',\n",
              " 'ni',\n",
              " 'nine',\n",
              " 'nisentiment_netsy',\n",
              " 'nj',\n",
              " 'nl',\n",
              " 'nn',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'non',\n",
              " 'none',\n",
              " 'nosentiment_netsheless',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'normally',\n",
              " 'nos',\n",
              " 'not',\n",
              " 'noted',\n",
              " 'nothing',\n",
              " 'novel',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'nr',\n",
              " 'ns',\n",
              " 'nt',\n",
              " 'ny',\n",
              " 'o',\n",
              " 'oa',\n",
              " 'ob',\n",
              " 'obtain',\n",
              " 'obtained',\n",
              " 'obviously',\n",
              " 'oc',\n",
              " 'od',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'og',\n",
              " 'oh',\n",
              " 'oi',\n",
              " 'oj',\n",
              " 'ok',\n",
              " 'okay',\n",
              " 'ol',\n",
              " 'old',\n",
              " 'om',\n",
              " 'omitted',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'ones',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'oo',\n",
              " 'op',\n",
              " 'oq',\n",
              " 'or',\n",
              " 'ord',\n",
              " 'os',\n",
              " 'ot',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'ou',\n",
              " 'ought',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'overall',\n",
              " 'ow',\n",
              " 'owing',\n",
              " 'own',\n",
              " 'ox',\n",
              " 'oz',\n",
              " 'p',\n",
              " 'p1',\n",
              " 'p2',\n",
              " 'p3',\n",
              " 'page',\n",
              " 'pagecount',\n",
              " 'pages',\n",
              " 'par',\n",
              " 'part',\n",
              " 'particular',\n",
              " 'particularly',\n",
              " 'pas',\n",
              " 'past',\n",
              " 'pc',\n",
              " 'pd',\n",
              " 'pe',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'pf',\n",
              " 'ph',\n",
              " 'pi',\n",
              " 'pj',\n",
              " 'pk',\n",
              " 'pl',\n",
              " 'placed',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'pm',\n",
              " 'pn',\n",
              " 'po',\n",
              " 'poorly',\n",
              " 'possible',\n",
              " 'possibly',\n",
              " 'potentially',\n",
              " 'pp',\n",
              " 'pq',\n",
              " 'pr',\n",
              " 'predominantly',\n",
              " 'present',\n",
              " 'presumably',\n",
              " 'previously',\n",
              " 'primarily',\n",
              " 'probably',\n",
              " 'promptly',\n",
              " 'proud',\n",
              " 'provides',\n",
              " 'ps',\n",
              " 'pt',\n",
              " 'pu',\n",
              " 'put',\n",
              " 'py',\n",
              " 'q',\n",
              " 'qj',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'quickly',\n",
              " 'quite',\n",
              " 'qv',\n",
              " 'r',\n",
              " 'r2',\n",
              " 'ra',\n",
              " 'ran',\n",
              " 'rather',\n",
              " 'rc',\n",
              " 'rd',\n",
              " 're',\n",
              " 'readily',\n",
              " 'really',\n",
              " 'reasonably',\n",
              " 'recent',\n",
              " 'recently',\n",
              " 'ref',\n",
              " 'refs',\n",
              " 'regarding',\n",
              " 'regardless',\n",
              " 'regards',\n",
              " 'related',\n",
              " 'relatively',\n",
              " 'research',\n",
              " 'research-articl',\n",
              " 'respectively',\n",
              " 'resulted',\n",
              " 'resulting',\n",
              " 'results',\n",
              " 'rf',\n",
              " 'rh',\n",
              " 'ri',\n",
              " 'right',\n",
              " 'rj',\n",
              " 'rl',\n",
              " 'rm',\n",
              " 'rn',\n",
              " 'ro',\n",
              " 'rq',\n",
              " 'rr',\n",
              " 'rs',\n",
              " 'rt',\n",
              " 'ru',\n",
              " 'run',\n",
              " 'rv',\n",
              " 'ry',\n",
              " 's',\n",
              " 's2',\n",
              " 'sa',\n",
              " 'said',\n",
              " 'same',\n",
              " 'saw',\n",
              " 'say',\n",
              " 'saying',\n",
              " 'says',\n",
              " 'sc',\n",
              " 'sd',\n",
              " 'se',\n",
              " 'sec',\n",
              " 'second',\n",
              " 'secondly',\n",
              " 'section',\n",
              " 'see',\n",
              " 'seeing',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'seen',\n",
              " 'self',\n",
              " 'selves',\n",
              " 'sensible',\n",
              " 'sent',\n",
              " 'serious',\n",
              " 'seriously',\n",
              " 'seven',\n",
              " 'several',\n",
              " 'sf',\n",
              " 'shall',\n",
              " 'shan',\n",
              " 'shan’t',\n",
              " 'she',\n",
              " 'shed',\n",
              " 'she’d',\n",
              " 'she’ll',\n",
              " 'shes',\n",
              " 'she’s',\n",
              " 'should',\n",
              " 'shouldn',\n",
              " 'shouldn’t',\n",
              " 'should’ve',\n",
              " 'show',\n",
              " 'showed',\n",
              " 'shown',\n",
              " 'showns',\n",
              " 'shows',\n",
              " 'si',\n",
              " 'side',\n",
              " 'significant',\n",
              " 'significantly',\n",
              " 'similar',\n",
              " 'similarly',\n",
              " 'since',\n",
              " 'sincere',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'sj',\n",
              " 'sl',\n",
              " 'slightly',\n",
              " 'sm',\n",
              " 'sn',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somebody',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'somethan',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhat',\n",
              " 'somewhere',\n",
              " 'soon',\n",
              " 'sorry',\n",
              " 'sp',\n",
              " 'specifically',\n",
              " 'specified',\n",
              " 'specify',\n",
              " 'specifying',\n",
              " 'sq',\n",
              " 'sr',\n",
              " 'ss',\n",
              " 'st',\n",
              " 'still',\n",
              " 'stop',\n",
              " 'strongly',\n",
              " 'sub',\n",
              " 'substantially',\n",
              " 'successfully',\n",
              " 'such',\n",
              " 'sufficiently',\n",
              " 'suggest',\n",
              " 'sup',\n",
              " 'sure',\n",
              " 'sy',\n",
              " 'system',\n",
              " 'sz',\n",
              " 't',\n",
              " 't1',\n",
              " 't2',\n",
              " 't3',\n",
              " 'take',\n",
              " 'taken',\n",
              " 'taking',\n",
              " 'tb',\n",
              " 'tc',\n",
              " 'td',\n",
              " 'te',\n",
              " 'tell',\n",
              " 'ten',\n",
              " 'tends',\n",
              " 'tf',\n",
              " 'th',\n",
              " 'than',\n",
              " 'thank',\n",
              " 'thanks',\n",
              " 'thanx',\n",
              " 'that',\n",
              " 'that’ll',\n",
              " 'thats',\n",
              " 'that’s',\n",
              " 'that’ve',\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'thered',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'there’ll',\n",
              " 'thereof',\n",
              " 'therere',\n",
              " 'theres',\n",
              " 'there’s',\n",
              " 'thereto',\n",
              " 'thereupon',\n",
              " 'there’ve',\n",
              " 'these',\n",
              " 'they',\n",
              " 'theyd',\n",
              " 'they’d',\n",
              " 'they’ll',\n",
              " 'theyre',\n",
              " 'they’re',\n",
              " 'they’ve',\n",
              " 'thickv',\n",
              " 'thin',\n",
              " 'think',\n",
              " 'third',\n",
              " 'this',\n",
              " 'thorough',\n",
              " 'thoroughly',\n",
              " 'those',\n",
              " 'thou',\n",
              " 'though',\n",
              " 'thoughh',\n",
              " 'thousand',\n",
              " 'three',\n",
              " 'throug',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'ti',\n",
              " 'til',\n",
              " 'tip',\n",
              " 'tj',\n",
              " 'tl',\n",
              " 'tm',\n",
              " 'tn',\n",
              " 'to',\n",
              " 'together',\n",
              " 'too',\n",
              " 'took',\n",
              " 'top',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'tp',\n",
              " 'tq',\n",
              " 'tr',\n",
              " 'tried',\n",
              " 'tries',\n",
              " 'truly',\n",
              " 'try',\n",
              " 'trying',\n",
              " 'ts',\n",
              " 't’s',\n",
              " 'tt',\n",
              " 'tv',\n",
              " 'twelve',\n",
              " 'twenty',\n",
              " 'twice',\n",
              " 'two',\n",
              " 'tx',\n",
              " 'u',\n",
              " 'u201d',\n",
              " 'ue',\n",
              " 'ui',\n",
              " 'uj',\n",
              " 'uk',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "stopwords2 = fix_stopword(stopwords_2)\n",
        "stopwords2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Append stopwords in nltk into stopwords2."
      ],
      "metadata": {
        "id": "Tx5zMqSv80Fu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PLyE6h1ZNya1"
      },
      "outputs": [],
      "source": [
        "for sw in stopwords:\n",
        "    if sw not in stopwords2:\n",
        "        stopwords2.append(sw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlFNSQ0d-Ycw"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "v-VYH68S-Ycw"
      },
      "outputs": [],
      "source": [
        "def case_folding(raw_input):\n",
        "    \"\"\" \n",
        "    Input harus berupa kolom dataframe \n",
        "    \"\"\"\n",
        "    return raw_input.lower()\n",
        "\n",
        "df_tweet.loc[:,'case_folding'] = df_tweet['tweet'].apply(lambda x : case_folding(x))\n",
        "\n",
        "def fix_slang(low_input):\n",
        "    sentence_list = low_input.split(' ')\n",
        "\n",
        "    # make a place where we can build our new sentence\n",
        "    new_sentence = []\n",
        "\n",
        "    # look through each word \n",
        "    for word in sentence_list:\n",
        "        # look for each candidate\n",
        "        for candidate_replacement in slangs:\n",
        "            # if our candidate is there in the word\n",
        "            if candidate_replacement == word:\n",
        "                # replace it \n",
        "                word = word.replace(candidate_replacement, slangs[candidate_replacement])\n",
        "\n",
        "        # and pop it onto a new list \n",
        "        new_sentence.append(word)\n",
        "\n",
        "    reformed = \" \".join(new_sentence)\n",
        "    return reformed\n",
        "\n",
        "df_tweet.loc[:,'fix_slang'] = df_tweet['case_folding'].apply(lambda x : fix_slang(x))\n",
        "\n",
        "def remove_stopword(text):\n",
        "    text = fix_slang(text)\n",
        "    text = text.replace(\"'\",\"’\")\n",
        "    words1 = \" \".join([w for w in text.split() if w not in stopwords_2])\n",
        "    words2 = \" \".join([w for w in words1.split() if w not in stopwords2])\n",
        "    return words2\n",
        "\n",
        "df_tweet.loc[:,'remove_stopword'] = df_tweet['fix_slang'].apply(lambda x : remove_stopword(x))\n",
        "\n",
        "def clean_tweet(text):\n",
        "    emojis = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
        "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U0001F1F2\"\n",
        "        u\"\\U0001F1F4\"\n",
        "        u\"\\U0001F620\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emojis.sub(r'', text)\n",
        "    text = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', ' ', str(text))\n",
        "    text = re.sub(\"/t|/b|\\n\",\" \", text)\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', ' ', str(text))\n",
        "    text = re.sub(r'#[A-Za-z0-9_]+', ' ', str(text))\n",
        "    text = re.sub('[0-9]+', ' ', text)\n",
        "    text = re.sub(r'RT : ', ' ', str(text))\n",
        "    text = re.sub(r'”|“|—|≈|↔|’', ' ', str(text))\n",
        "    text = text.replace('…',' ').replace('..',' ')\n",
        "    text  = \"\".join([char for char in text if char not in string.punctuation]) # remove !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
        "    text = text.replace('northernisland','northern island') \n",
        "    text = text.replace('sundaythought','sunday thought') \n",
        "    text = text.replace('punjabelect','punjab elect') \n",
        "    text = text.replace('vaccines”','vaccines') \n",
        "    text = text.replace('immunocompromis','immun compromis') \n",
        "    text = text.replace('sundayvib','sunday vib') \n",
        "    text = text.replace('omicronfuel','omicron fuel') \n",
        "    text = text.replace('vaccinemand','vaccine mand') \n",
        "    text = text.replace('nofleecovidofdeatharriv','no flee covid of death arriv') \n",
        "    text = text.replace('“experts”','experts') \n",
        "    text = text.replace('breakingbrekingnew','breaking new') \n",
        "    text = text.replace('mengwanzhou','men gwanzhou') \n",
        "    text = text.replace('theliberti','the liberti') \n",
        "    text = text.replace('masterokawa','master okawa') \n",
        "    text = text.replace('ryuhookawa','ryuho okawa') \n",
        "    text = text.replace('आरहीहैcongress','congress') \n",
        "    text = text.replace('knowomicron','know omicron') \n",
        "    text = text.replace('sundaymorn','sunday morn') \n",
        "    text = text.replace('getvaccinatedandboost','get vaccinated and boost') \n",
        "    text = text.replace('misconstru','mis constru') \n",
        "    text = text.replace('omicrondriven','omicron driven') \n",
        "    text = text.replace('commonplac','common plac') \n",
        "    text = text.replace('knowomicron','know omicron') \n",
        "    text = text.replace('🤗', '') \n",
        "    text = text.replace('🥺', '') \n",
        "    text = text.replace('midjanuari', 'mid januari') \n",
        "    text = text.replace('hospitalbas', 'hospital bas') \n",
        "    text = text.replace('getboost', 'get boost') \n",
        "    text = text.replace('perthnew', 'perth new') \n",
        "    text = text.replace('nvax', '') \n",
        "    text = text.replace('datastori', 'data stori') \n",
        "    text = text.replace('…the', 'the') \n",
        "    text = text.replace('amppassport', 'amp passport') \n",
        "    text = text.replace('bbcbreakfast', 'bbc breakfast') \n",
        "    text = text.replace('🤣', '') \n",
        "    text = text.replace('sharescoronavirus', 'shares coronavirus') \n",
        "    text = text.replace('letitrip', 'let it rip') \n",
        "    text = text.replace('southafricas', 'south africas') \n",
        "    text = text.replace('fortressaustralia', 'fortress australia') \n",
        "    text = text.replace('lightningfast', 'lightning fast') \n",
        "    text = text.replace('vaccinationinfect', 'vaccination infect') \n",
        "    text = text.replace('coronaviruspandem', 'coronavirus pandem') \n",
        "    text = text.replace('bioinformat', 'bio informat') \n",
        "    text = text.replace('peoplesconvoy', 'peoples convoy') \n",
        "    text = text.replace('scottydoesnoth', 'scotty does not') \n",
        "    text = text.replace('asap', 'as soon as possible') \n",
        "    text = text.replace('infectiousx', 'infectious') \n",
        "    text = text.replace('bedridden', 'bed ridden') \n",
        "    text = text.replace('illnessdeath', 'illness death') \n",
        "    text = text.replace('antivaxxerscovid', 'antivaxxers covid') \n",
        "    text = text.replace('covidzero', 'covid zero') \n",
        "    text = text.replace('tnnew', 'new') \n",
        "    text = text.replace('cabbook', 'cab book') \n",
        "    text = text.replace('rentalcar', 'rental car') \n",
        "    text = text.replace('provax', 'pro vax') \n",
        "    text = text.replace('joketh', 'joke th') \n",
        "    text = text.replace('judgmentand', 'judgment and') \n",
        "    text = text.replace('bananimalsacrific', 'ban animals sacrific') \n",
        "    text = text.replace('banliveanimalmarket', 'ban live animal market') \n",
        "    text = text.replace('animalprotect', 'animal_protect') \n",
        "    text = text.replace('banthedogandcatmeattrad', 'ban the dog and cat meat trad') \n",
        "    text = text.replace('lovedonesnotnumb', 'love does not numb') \n",
        "    text = text.replace('coldscoughsfev', 'colds coughs fev') \n",
        "    text = text.replace('doctorssay', 'doctors say') \n",
        "    text = text.replace('internationalnew', 'international new') \n",
        "    text = text.replace('fatigueey', 'fatigue') \n",
        "    text = text.replace('fogheadach', 'fog headach') \n",
        "    text = text.replace('releasesoheila', 'release soheila') \n",
        "    text = text.replace('freeraheleh', 'free raheleh') \n",
        "    text = text.replace('danandrew', 'dan andrew') \n",
        "    text = text.replace('longcovid', 'long covid') \n",
        "    text = text.replace('omicronsinc', 'omicron sinc') \n",
        "    text = text.replace('withfrom', 'with from') \n",
        "    text = text.replace('mildtomoder', 'mild to moder') \n",
        "    text = text.replace('defensesfutur', 'defenses futur') \n",
        "    text = text.replace('ccpvirus', 'ccp virus') \n",
        "    text = text.replace('atrisk', 'at risk') \n",
        "    text = text.replace('mama🤒', 'mama') \n",
        "    text = text.replace('omicronba', 'omicron ba') \n",
        "    text = text.replace('covidvaccinedisast', 'covid vaccine disast') \n",
        "    text = text.replace('datarememb', 'data rememb') \n",
        "    text = text.replace('vigilancenot', 'vigilancenot') \n",
        "    text = text.replace('healthministri', 'health ministri') \n",
        "    text = text.replace('🤖', ' ') \n",
        "    text = text.replace('🧑', ' ') \n",
        "    text = text.replace('detailsappli', 'details appli') \n",
        "    text = text.replace('natureorigin', 'nature origin') \n",
        "    text = text.replace('discussioncovid', 'discussion covid') \n",
        "    text = text.replace('antibodiesth', 'antibodies th') \n",
        "    text = text.replace('withinhost', 'within host') \n",
        "    text = text.replace('nonhealth', 'non health') \n",
        "    text = text.replace('🥤', ' ') \n",
        "    text = text.replace('largestvaccinedr', 'largest vaccine dr ') \n",
        "    text = text.replace('unitefightcorona' , 'unite fight corona') \n",
        "    text = text.replace('prepandem', 'pre pandem ') \n",
        "    text = text.replace('wildstrain', 'wild strain') \n",
        "    text = text.replace('harborview', 'harbor view') \n",
        "    text = text.replace('gamesomicron', 'games omicron') \n",
        "    text = text.replace('typeofvaccine', 'type of vaccine') \n",
        "    text = text.replace('thatgivescellularimmunitybillg', 'that gives cellular immunity bill g') \n",
        "    text = text.replace('appomicronspreadingfasterthanvaccinestogiveimmunitybillgateshtmldownload', 'app omicrons preading faster than vaccines to give immunity bill gates') \n",
        "    text = text.replace('berhenti', ' ') \n",
        "    text = text.replace('membaca', ' ') \n",
        "    text = text.replace('cerita', ' ') \n",
        "    text = text.replace('konspirasi', ' ') \n",
        "    text = text.replace('sampah', ' ') \n",
        "    text = text.replace('fb', ' ') \n",
        "    text = text.replace('lindungi', ' ') \n",
        "    text = text.replace('diri', ' ') \n",
        "    text = text.replace('dapatkan', ' ') \n",
        "    text = text.replace('dos', ' ') \n",
        "    text = text.replace('penggalak', ' ') \n",
        "    text = text.replace('wuhanalphadelta', 'wuhan alpha delta ') \n",
        "    text = text.replace('omicronsmash', 'omicron smash') \n",
        "    text = text.replace('covidomicron', 'covid omicron') \n",
        "    text = text.replace('nonessenti', 'non essenti') \n",
        "    text = text.replace('wuhanvirus', 'wuhan virus') \n",
        "    text = text.replace('attackbut', 'attack but') \n",
        "    text = text.replace('denselypopul', 'densely popul') \n",
        "    text = text.replace('miamidad', 'miami dad') \n",
        "    text = text.replace('floridabut', 'florida but') \n",
        "    text = text.replace('midcentr', 'mid centr') \n",
        "    text = text.replace('thatgivescellularimmunitybillg', 'that gives cellular immunity bill g') \n",
        "    text = text.replace('governanceimpl', 'governance impl') \n",
        "    text = text.replace('businesstravel', 'business travel') \n",
        "    text = text.replace('digitalnomad', 'digital nomad') \n",
        "    text = text.replace('travelrestrict', 'travel restrict') \n",
        "    text = text.replace('healthinsur', 'health insur') \n",
        "    text = text.replace('isolatingth', 'isolating th') \n",
        "    text = text.replace('breakingnew', 'breaking new') \n",
        "    text = text.replace('passes🤞w', 'passes') \n",
        "    text = text.replace('cases🤔', 'cases') \n",
        "    text = text.replace('nlandcouncil', 'n land council') \n",
        "    text = text.replace('worldlockdown', 'world lockdown') \n",
        "    text = text.replace('şıonlin', ' ') \n",
        "    text = text.replace('dominance🧵', 'dominance') \n",
        "    text = text.replace('billionbaht', 'billion baht') \n",
        "    text = text.replace('ประเทศไทย', ' ') \n",
        "    text = text.replace('4314 ประกันสังคม', ' ') \n",
        "    text = text.replace('4315 โควิด', ' ') \n",
        "    text = text.replace('4316 โอไมครอน', ' ') \n",
        "    text = text.replace('genome🧬', 'genome') \n",
        "    text = text.replace('4338 cluea', 'clue') \n",
        "    text = text.replace('4341 grandchildren', 'grandchildren') \n",
        "    text = text.replace('boosterprevent', 'booster prevent') \n",
        "    text = text.replace('🟨', ' ') \n",
        "    text = text.replace(r'🤔', ' ') \n",
        "    text = text.replace(r'🥴', ' ') \n",
        "    text = text.replace(r'🤡', ' ') \n",
        "    text = text.replace('distributingit', 'distributing it') \n",
        "    text = text.replace('restaurantsbut', 'restaurants but') \n",
        "    text = text.replace('mistakemor', 'mistake mor') \n",
        "    text = text.replace('infectiondiseas', 'infection diseas') \n",
        "    text = text.replace('triplevaxx', 'triple vaccinated') \n",
        "    text = text.replace('spijtig', ' ') \n",
        "    text = text.replace('horsemount', 'horse mount') \n",
        "    text = text.replace('🤥', ' ') \n",
        "    text = text.replace('withwhat', 'with what') \n",
        "    text = text.replace('omicronhappi', 'omicron happi') \n",
        "    text = text.replace('4685 boostermight', 'booster might') \n",
        "    text = text.replace('fyi', 'for your information') \n",
        "    text = text.replace('crazyth', 'crazy') \n",
        "    text = text.replace('coldflu', 'cold flu') \n",
        "    text = text.replace('powerhungri', 'power hungri') \n",
        "    text = text.replace('journalcouri', 'journal couri') \n",
        "    text = text.replace('pendingvir', 'pending vir') \n",
        "    text = text.replace('wvstatejourn', 'wv state journ') \n",
        "    text = text.replace('apparentlag', 'apparent lag') \n",
        "    text = text.replace('covidconfid', 'covid confid') \n",
        "    text = text.replace('campground', 'camp ground') \n",
        "    text = text.replace('countermeasur', 'counter measur') \n",
        "    text = text.replace('quotetweet', 'quotet weet') \n",
        "    text = text.replace('offfuck', 'off fuck') \n",
        "    text = text.replace('omicronmildnessnatur', 'omicron mildness natur')\n",
        "    return text\n",
        "\n",
        "df_tweet.loc[:,'cleaned_tweet'] = df_tweet['remove_stopword'].apply(lambda x : clean_tweet(x))\n",
        "\n",
        "def text_stemmer(text):\n",
        "    words = text.split()\n",
        "    choice = SnowballStemmer('english')\n",
        "    reformed = [choice.stem(word) for word in words]\n",
        "    reformed = \" \".join(reformed)\n",
        "    return reformed\n",
        "\n",
        "df_tweet.loc[:,'text_stemmer'] = df_tweet['cleaned_tweet'].apply(lambda x: text_stemmer(x))\n",
        "df_tweet.loc[:,'text_stemmer'] = df_tweet['text_stemmer'].apply(lambda x : x.replace(\"'\",\" \"))\n",
        "token_tweet = df_tweet['text_stemmer'].apply(lambda x: x.split())\n",
        "df_tweet.loc[:,'tokenized'] = token_tweet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(tweets_ints):\n",
        "  ''' Return features of review_ints, where each review is padded with 0's \n",
        "      or truncated to the input seq_length.\n",
        "  '''\n",
        "  tweet_length = []\n",
        "  for token_words in tweets_ints:\n",
        "    tweet_length.append(len(token_words))\n",
        "  ## getting the correct rows x cols shape\n",
        "  features = np.zeros((len(tweets_ints), max(tweet_length)), dtype=int)\n",
        "  \n",
        "  ## for each review, I grab that review\n",
        "  for i, row in enumerate(tweets_ints):\n",
        "    features[i, -len(row):] = np.array(row)[:max(tweet_length)]\n",
        "  \n",
        "  return features"
      ],
      "metadata": {
        "id": "z8Q4oB6rrQ0z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = pad_features(tokenized_tweet)\n",
        "## test statements - do not change - ##\n",
        "tweet_length = []\n",
        "for token_words in tokenized_tweet:\n",
        "  tweet_length.append(len(token_words))\n",
        "\n",
        "assert len(features)==len(tokenized_tweet), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==max(tweet_length), \"Each feature row should contain seq_length values.\"\n",
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMQU4fwNrRnS",
        "outputId": "cc963791-d372-4d0e-8778-0b77b203a5ce"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[     0      0      0 ...     33  11198   1939]\n",
            " [     0      0      0 ...   1767   1282 748824]\n",
            " [     0      0      0 ...   1086 478855    932]\n",
            " ...\n",
            " [     0      0      0 ... 201023  34984 913236]\n",
            " [     0      0      0 ...    175    330  51406]\n",
            " [     0      0      0 ...   1815  34883 843449]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFe8wFXwGyCN"
      },
      "source": [
        "# Edit glove_model\n",
        "\n",
        "Here, we append new index that represents the word '<pad>'. This is also the word represent unknown words or words that doesnt exists in glove model. \n",
        "\n",
        "We also set the value of this word into a 100-D vector of zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qB_x8GFbLMwN"
      },
      "outputs": [],
      "source": [
        "glove_model.stoi['<pad>'] = 1193514"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OekNLpfaLTY2"
      },
      "outputs": [],
      "source": [
        "glove_model.vectors = torch.cat((glove_model.vectors,torch.zeros([1,100])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPm1AMN923vP"
      },
      "source": [
        "# Train and Test Split\n",
        "\n",
        "In this step, we split the data into train, validation, and test data with the ratio of 75% : 15%: 5%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6usFZDbH8mnu"
      },
      "outputs": [],
      "source": [
        "data_1000 = padded_tweet[:]\n",
        "i = 0\n",
        "while i < len(data_1000):\n",
        "    data_1000[i] = torch.atleast_2d(torch.LongTensor([glove_model.stoi[char] for char in data_1000[i]]))\n",
        "    i += 1\n",
        "label = []\n",
        "for data in df_tweet['sentiment']:\n",
        "    label.append(data)\n",
        "label = torch.Tensor(label)\n",
        "data_sets = []\n",
        "for p,enc in zip(data_1000,label):\n",
        "  data_sets.append((p,enc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1OJDLxzJLgMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc159b17-4071-4473-ca39-d80fc003eb1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tFeatures Shapes:\n",
            "Train set: \t\ttorch.Size([500, 32]) \n",
            "Validation set: \ttorch.Size([445, 32]) \n",
            "Test set: \t\ttorch.Size([55, 32])\n"
          ]
        }
      ],
      "source": [
        "split_frac_3 = 0.5\n",
        "split_idx_3 = int(len(data_1000)*split_frac_3)\n",
        "train_x_70, remaining_x_70 = data_1000[:split_idx_3], data_1000[split_idx_3:]\n",
        "train_y_70, remaining_y_70 = label[:split_idx_3], label[split_idx_3:]\n",
        "\n",
        "# Test dan val berada dalam satu proporsi yang sama\n",
        "# Referensi : https://www.kaggle.com/code/alvations/nlp-data-splits/notebook\n",
        "test_idx = int(len(remaining_x_70)*0.89)\n",
        "val_x_70, test_x_70 = remaining_x_70[:test_idx], remaining_x_70[test_idx:]\n",
        "val_y_70, test_y_70 = remaining_y_70[:test_idx], remaining_y_70[test_idx:]\n",
        "\n",
        "train_x_70 = torch.cat(train_x_70)\n",
        "remaining_x_70 = torch.cat(remaining_x_70)\n",
        "val_x_70 = torch.cat(val_x_70)\n",
        "test_x_70= torch.cat(test_x_70)\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeatures Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x_70.size()),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x_70.size()),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x_70.size()))\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data_70 = TensorDataset(train_x_70, train_y_70)\n",
        "valid_data_70 = TensorDataset(val_x_70, val_y_70)\n",
        "test_data_70 = TensorDataset(test_x_70, test_y_70)\n",
        "\n",
        "batch = 64\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "# Masukkan data loader ke GPU\n",
        "train_loader = DataLoader(train_data_70, shuffle=True, batch_size = batch, num_workers=0)\n",
        "valid_loader = DataLoader(valid_data_70, shuffle=True, batch_size = batch, num_workers=0)\n",
        "test_loader = DataLoader(test_data_70, shuffle=True, batch_size = batch, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1wNx3zPGccW"
      },
      "source": [
        "# Fully Connected\n",
        "\n",
        "Fully connected layer is a layer that transform the dimensional of a data so it can be classified linearly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Jd3J6DHGGeZc"
      },
      "outputs": [],
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, input_size, output_size, bias=True):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.bias = bias\n",
        "        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size))\n",
        "        self.bias = torch.nn.Parameter(torch.randn(output_size))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out()\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            return torch.nn.init.uniform_(self.bias, -bound, bound)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x, y = input.size()\n",
        "        if y != self.input_size:\n",
        "            print(f'Ukuran input salah. Gunakan ukuran input sebesar {self.input_size}')\n",
        "            return 0\n",
        "        output = input @ self.weight.t() #y = W*x + b\n",
        "        if self.bias is not None:\n",
        "            output += self.bias\n",
        "            return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid"
      ],
      "metadata": {
        "id": "QE6JJqOlDaNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(nn.Module):\n",
        "    # Forward pass\n",
        "  def __init__(self , derived = True):\n",
        "    super(Sigmoid,self).__init__()\n",
        "    # Save input and calculate/save output\n",
        "    # of the sigmoid function\n",
        "    self.derived = derived\n",
        "  \n",
        "  def sigmoid(self,x):\n",
        "    with torch.no_grad():\n",
        "      if not self.derived:\n",
        "        return 1 / (1 + torch.exp(-x))\n",
        "      else :\n",
        "        sigm = 1 / (1 + torch.exp(-x))\n",
        "        d_sigm = sigm * (1-sigm)\n",
        "        return d_sigm"
      ],
      "metadata": {
        "id": "mrXt0vufDbNZ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TanH"
      ],
      "metadata": {
        "id": "oqL1NmedDcZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TanH(nn.Module):\n",
        "  \n",
        "  def forward(self, x, derivative = True):\n",
        "    super(TanH,self).__init__()\n",
        "    self.x = x\n",
        "    # Forward\n",
        "    with torch.no_grad():\n",
        "      if not derivative:\n",
        "        output = (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
        "        return output\n",
        "    # Backward pass\n",
        "    # Derivative - calculates from output of the tanh function\n",
        "      else :\n",
        "        output = (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
        "        d_out = 1 - (output)**2\n",
        "        return d_out"
      ],
      "metadata": {
        "id": "s58D6_k2Db_p"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8lHWoqcApLL"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTt7oXJ4nbZJ"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NvHd5MyXqEYY"
      },
      "outputs": [],
      "source": [
        "sigmoid = Sigmoid()\n",
        "tanh = TanH()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "O8Jd25XPrVhM"
      },
      "outputs": [],
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim: int, hidden_size: int):\n",
        "        super(SentimentLSTM,self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Inisialisasi bobot dan bias\n",
        "        # Source : https://github.com/piEsposito/pytorch-lstm-by-hand/blob/master/nlp-naive-lstm-byhand.ipynb\n",
        "\n",
        "        # forget gate\n",
        "        self.uf = nn.Parameter(torch.Tensor(self.embedding_dim, self.hidden_size))\n",
        "        self.vf = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
        "        self.bf = nn.Parameter(torch.zeros(self.hidden_size))\n",
        "\n",
        "        # input gate\n",
        "        self.ui = nn.Parameter(torch.Tensor(self.embedding_dim, self.hidden_size))\n",
        "        self.vi = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
        "        self.bi = nn.Parameter(torch.zeros(self.hidden_size))\n",
        "\n",
        "        # candidate gate\n",
        "        self.uc = nn.Parameter(torch.Tensor(self.embedding_dim, self.hidden_size))\n",
        "        self.vc = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
        "        self.bc = nn.Parameter(torch.zeros(self.hidden_size))\n",
        "\n",
        "        # output gate\n",
        "        self.uo = nn.Parameter(torch.Tensor(self.embedding_dim, self.hidden_size))\n",
        "        self.vo = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
        "        self.bo = nn.Parameter(torch.zeros(self.hidden_size))\n",
        "\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "      stdv = 1.0/math.sqrt(self.hidden_size)\n",
        "      for weight in self.parameters():\n",
        "        weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Asumsi shape dari input adalah (batch_size, sequence_length, input_size/ embedding_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, embedding_dim = input.size()\n",
        "        hidden_seq,cell_state,f_t,i_t,g_t,o_t = [],[],[],[],[],[]\n",
        "        lstm_output_dict = {}\n",
        "\n",
        "\n",
        "        h_t, c_t = (torch.zeros(batch_size, self.hidden_size).to(device), \n",
        "                    torch.zeros(batch_size, self.hidden_size).to(device))\n",
        "\n",
        "        hidden_seq.append(h_t.unsqueeze(0))\n",
        "        cell_state.append(c_t.unsqueeze(0))\n",
        "    \n",
        "        for w in range(seq_length):\n",
        "            xt = input[:,w,:]\n",
        "            it = sigmoid(xt @ self.ui + h_t @ self.vi + self.bi)\n",
        "            i_t.append(it.unsqueeze(0))\n",
        "            ft = sigmoid(xt @ self.uf + h_t @ self.vf + self.bf)\n",
        "            f_t.append(ft.unsqueeze(0))\n",
        "            gt = tanh(xt @ self.uc + h_t @ self.vc + self.bc)\n",
        "            g_t.append(gt.unsqueeze(0))\n",
        "            ot = sigmoid(xt @ self.uo + h_t @ self.vo + self.bo)\n",
        "            o_t.append(ot.unsqueeze(0))\n",
        "            c_t = ft * c_t + it * gt\n",
        "            cell_state.append(c_t.unsqueeze(0))\n",
        "            h_t = ot * torch.tanh(c_t)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "        \n",
        "        # Reshape\n",
        "        lstm_output_dict['ht'] = torch.cat(hidden_seq, dim=0).transpose(0,1).contiguous()\n",
        "        lstm_output_dict['ct'] = torch.cat(cell_state, dim=0).transpose(0,1).contiguous()\n",
        "        lstm_output_dict['ft'] = torch.cat(f_t, dim=0).transpose(0,1).contiguous()\n",
        "        lstm_output_dict['it'] = torch.cat(i_t, dim=0).transpose(0,1).contiguous()\n",
        "        lstm_output_dict['gt'] = torch.cat(g_t, dim=0).transpose(0,1).contiguous()\n",
        "        lstm_output_dict['ot'] = torch.cat(o_t, dim=0).transpose(0,1).contiguous()\n",
        "        return lstm_output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv3HMSoWJOUe"
      },
      "source": [
        "## Sentiment Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "HthAY2PQnIgo"
      },
      "outputs": [],
      "source": [
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, output_size):\n",
        "        super(SentimentNet,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding.from_pretrained(glove_model.vectors,freeze=False)\n",
        "        self.lstm = SentimentLSTM(embedding_size,self.hidden_size)\n",
        "        self.linear = FullyConnected(self.hidden_size,output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Network Forward Pass\n",
        "        \"\"\"\n",
        "        net_out = dict()\n",
        "        x = self.embedding(input)\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        out_dict = self.lstm(x)\n",
        "        out_final = out_dict['ht']\n",
        "\n",
        "        # flattened lstm output\n",
        "        out_final = out_final.contiguous().view(-1,self.hidden_size)\n",
        "\n",
        "        out_final = self.linear(out_final)\n",
        "        out_final = self.sigmoid(out_final).view(batch_size,-1)[:,-1]\n",
        "\n",
        "        # Insert to dictionary\n",
        "        net_out['network_output'] = out_final\n",
        "        net_out['ft'] = out_dict['ft']\n",
        "        net_out['it'] = out_dict['it']\n",
        "        net_out['gt'] = out_dict['gt']\n",
        "        net_out['ot'] = out_dict['ot']\n",
        "        net_out['ct'] = out_dict['ct']\n",
        "        net_out['ht'] = out_dict['ht']\n",
        "        \n",
        "        return net_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JQMVjFtfkmVf"
      },
      "outputs": [],
      "source": [
        "def get_learning_rate(optimizer):\n",
        "  for params in optimizer.param_groups:\n",
        "    return params['lr']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIWm7UStZLLH"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "D1Gi_uyy9RzD"
      },
      "outputs": [],
      "source": [
        "bce_loss = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TzTt1zbRT2Wo"
      },
      "outputs": [],
      "source": [
        "def training_epoch(model, dataloader, optimizer, scheduler, clip):\n",
        "  train_loss = 0.0\n",
        "  train_correct = 0\n",
        "  lrs = []\n",
        "  model.train()\n",
        "  for inputs, targets in dataloader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "    output = model.forward(inputs)\n",
        "    loss = bce_loss(output['network_output'],targets)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    pred = torch.round(output['network_output'])\n",
        "    train_correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "\n",
        "  return train_loss, train_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hsnpIMn-p5qF"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validating_epoch(model, dataloader):\n",
        "  valid_loss = 0.0\n",
        "  valid_correct = 0\n",
        "  model.eval()\n",
        "  for inputs, targets in dataloader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    output = model.forward(inputs)\n",
        "    loss = bce_loss(output['network_output'],targets)\n",
        "    valid_loss += loss.item()\n",
        "    pred = torch.round(output['network_output'])\n",
        "    valid_correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    \n",
        "  return valid_loss,valid_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WMkvVrcWvO-g"
      },
      "outputs": [],
      "source": [
        "def model_fit(num_epochs, model, train_loader, val_loader, optimizer, scheduler, clip_value, best_val_acc = 0):\n",
        "  \n",
        "  record = {'epoch':[],'train_loss': [], 'val_loss': [],'train_acc': [],'val_acc': []}\n",
        "  \n",
        "  best_val_acc = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss, train_correct = training_epoch(model,train_loader,optimizer, scheduler, clip_value)\n",
        "    val_loss, val_correct = validating_epoch(model,val_loader)\n",
        "  \n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_acc = train_correct / len(train_loader.dataset) * 100\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    val_acc = val_correct / len(val_loader.dataset) * 100\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "      print(f'Validation Accuracy Increased({best_val_acc:}--->{val_acc:}) \\t Menyimpan Model')\n",
        "      best_val_acc = val_acc\n",
        "      # Saving State Dict\n",
        "      torch.save(model.state_dict(), 'best_model_sentiment_nets.pt')\n",
        "  \n",
        "    print(\"Epoch:{}/{} AVG Training Loss:{:.6f} AVG Val Loss:{:.6f} AVG Training Acc {:.2f} % AVG Val Acc {:.2f} %\".format(epoch + 1,\\\n",
        "                                          num_epochs, train_loss, val_loss, train_acc, val_acc))\n",
        "\n",
        "    record['epoch'].append(epoch+1)\n",
        "    record['train_loss'].append(train_loss)\n",
        "    record['val_loss'].append(val_loss)\n",
        "    record['train_acc'].append(round(train_acc,2))\n",
        "    record['val_acc'].append(round(val_acc,2))\n",
        "\n",
        "  return pd.DataFrame(record)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Train Model"
      ],
      "metadata": {
        "id": "83vvxLphBT10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "OE54hzp6neGY"
      },
      "outputs": [],
      "source": [
        "# training params\n",
        "embedding_size = 100\n",
        "hidden_size = 128\n",
        "output_size = 1\n",
        "model = SentimentNet(embedding_size,hidden_size,output_size).to(device)\n",
        "num_epochs = 50\n",
        "clip = 1\n",
        "lr = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
        "                                               lr, epochs=num_epochs, steps_per_epoch=len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "A8BPjrdTnepu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497b257d-82bb-435c-86f8-77df2b10c3b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy Increased(0--->51.91011235955057) \t Menyimpan Model\n",
            "Epoch:1/50 AVG Training Loss:0.817615 AVG Val Loss:0.770450 AVG Training Acc 53.40 % AVG Val Acc 51.91 %\n",
            "Validation Accuracy Increased(51.91011235955057--->55.955056179775276) \t Menyimpan Model\n",
            "Epoch:2/50 AVG Training Loss:0.710177 AVG Val Loss:0.715219 AVG Training Acc 56.40 % AVG Val Acc 55.96 %\n",
            "Epoch:3/50 AVG Training Loss:0.617633 AVG Val Loss:0.748508 AVG Training Acc 66.20 % AVG Val Acc 55.51 %\n",
            "Validation Accuracy Increased(55.955056179775276--->58.20224719101124) \t Menyimpan Model\n",
            "Epoch:4/50 AVG Training Loss:0.541095 AVG Val Loss:0.716900 AVG Training Acc 72.60 % AVG Val Acc 58.20 %\n",
            "Epoch:5/50 AVG Training Loss:0.442381 AVG Val Loss:0.813796 AVG Training Acc 79.00 % AVG Val Acc 55.51 %\n",
            "Epoch:6/50 AVG Training Loss:0.318420 AVG Val Loss:1.014209 AVG Training Acc 87.80 % AVG Val Acc 56.40 %\n",
            "Epoch:7/50 AVG Training Loss:0.183448 AVG Val Loss:1.556818 AVG Training Acc 94.20 % AVG Val Acc 54.16 %\n",
            "Epoch:8/50 AVG Training Loss:0.095857 AVG Val Loss:2.004043 AVG Training Acc 96.00 % AVG Val Acc 52.13 %\n",
            "Epoch:9/50 AVG Training Loss:0.103250 AVG Val Loss:1.683846 AVG Training Acc 97.00 % AVG Val Acc 55.28 %\n",
            "Epoch:10/50 AVG Training Loss:0.062301 AVG Val Loss:2.012167 AVG Training Acc 97.80 % AVG Val Acc 57.08 %\n",
            "Epoch:11/50 AVG Training Loss:0.054572 AVG Val Loss:1.940305 AVG Training Acc 98.40 % AVG Val Acc 55.06 %\n",
            "Epoch:12/50 AVG Training Loss:0.048879 AVG Val Loss:1.812117 AVG Training Acc 97.80 % AVG Val Acc 56.18 %\n",
            "Epoch:13/50 AVG Training Loss:0.041843 AVG Val Loss:2.132870 AVG Training Acc 98.20 % AVG Val Acc 56.40 %\n",
            "Epoch:14/50 AVG Training Loss:0.038051 AVG Val Loss:2.359111 AVG Training Acc 98.20 % AVG Val Acc 55.51 %\n",
            "Epoch:15/50 AVG Training Loss:0.048232 AVG Val Loss:2.010581 AVG Training Acc 98.60 % AVG Val Acc 53.48 %\n",
            "Epoch:16/50 AVG Training Loss:0.027033 AVG Val Loss:2.213512 AVG Training Acc 98.80 % AVG Val Acc 53.93 %\n",
            "Epoch:17/50 AVG Training Loss:0.030953 AVG Val Loss:2.302043 AVG Training Acc 97.80 % AVG Val Acc 54.83 %\n",
            "Epoch:18/50 AVG Training Loss:0.028219 AVG Val Loss:2.613099 AVG Training Acc 98.80 % AVG Val Acc 52.13 %\n",
            "Epoch:19/50 AVG Training Loss:0.032167 AVG Val Loss:1.702536 AVG Training Acc 98.60 % AVG Val Acc 55.73 %\n",
            "Epoch:20/50 AVG Training Loss:0.031415 AVG Val Loss:2.234170 AVG Training Acc 98.20 % AVG Val Acc 55.28 %\n",
            "Epoch:21/50 AVG Training Loss:0.027721 AVG Val Loss:2.758290 AVG Training Acc 98.20 % AVG Val Acc 54.83 %\n",
            "Epoch:22/50 AVG Training Loss:0.020682 AVG Val Loss:2.691423 AVG Training Acc 98.80 % AVG Val Acc 54.38 %\n",
            "Epoch:23/50 AVG Training Loss:0.018313 AVG Val Loss:2.869063 AVG Training Acc 98.80 % AVG Val Acc 54.38 %\n",
            "Epoch:24/50 AVG Training Loss:0.015613 AVG Val Loss:3.346754 AVG Training Acc 99.20 % AVG Val Acc 54.38 %\n",
            "Epoch:25/50 AVG Training Loss:0.016581 AVG Val Loss:3.343029 AVG Training Acc 98.80 % AVG Val Acc 52.58 %\n",
            "Epoch:26/50 AVG Training Loss:0.022800 AVG Val Loss:2.611459 AVG Training Acc 98.40 % AVG Val Acc 53.26 %\n",
            "Epoch:27/50 AVG Training Loss:0.025133 AVG Val Loss:2.557151 AVG Training Acc 98.60 % AVG Val Acc 53.48 %\n",
            "Epoch:28/50 AVG Training Loss:0.021468 AVG Val Loss:2.921116 AVG Training Acc 99.00 % AVG Val Acc 53.48 %\n",
            "Epoch:29/50 AVG Training Loss:0.019554 AVG Val Loss:3.436535 AVG Training Acc 99.00 % AVG Val Acc 53.48 %\n",
            "Epoch:30/50 AVG Training Loss:0.020363 AVG Val Loss:4.423328 AVG Training Acc 99.00 % AVG Val Acc 51.91 %\n",
            "Epoch:31/50 AVG Training Loss:0.017929 AVG Val Loss:4.787306 AVG Training Acc 99.00 % AVG Val Acc 52.36 %\n",
            "Epoch:32/50 AVG Training Loss:0.018334 AVG Val Loss:4.866070 AVG Training Acc 99.00 % AVG Val Acc 51.91 %\n",
            "Epoch:33/50 AVG Training Loss:0.018963 AVG Val Loss:5.163713 AVG Training Acc 99.00 % AVG Val Acc 52.81 %\n",
            "Epoch:34/50 AVG Training Loss:0.018373 AVG Val Loss:5.647683 AVG Training Acc 99.00 % AVG Val Acc 52.58 %\n",
            "Epoch:35/50 AVG Training Loss:0.018607 AVG Val Loss:5.754366 AVG Training Acc 99.00 % AVG Val Acc 52.36 %\n",
            "Epoch:36/50 AVG Training Loss:0.016900 AVG Val Loss:5.710562 AVG Training Acc 99.00 % AVG Val Acc 53.48 %\n",
            "Epoch:37/50 AVG Training Loss:0.015897 AVG Val Loss:7.112958 AVG Training Acc 99.00 % AVG Val Acc 54.38 %\n",
            "Epoch:38/50 AVG Training Loss:0.014720 AVG Val Loss:6.315268 AVG Training Acc 99.20 % AVG Val Acc 54.83 %\n",
            "Epoch:39/50 AVG Training Loss:0.013866 AVG Val Loss:6.474566 AVG Training Acc 99.00 % AVG Val Acc 55.06 %\n",
            "Epoch:40/50 AVG Training Loss:0.012684 AVG Val Loss:6.499362 AVG Training Acc 98.80 % AVG Val Acc 54.83 %\n",
            "Epoch:41/50 AVG Training Loss:0.013141 AVG Val Loss:6.682570 AVG Training Acc 99.00 % AVG Val Acc 54.61 %\n",
            "Epoch:42/50 AVG Training Loss:0.012639 AVG Val Loss:6.722613 AVG Training Acc 99.20 % AVG Val Acc 54.61 %\n",
            "Epoch:43/50 AVG Training Loss:0.012465 AVG Val Loss:6.950079 AVG Training Acc 99.20 % AVG Val Acc 54.38 %\n",
            "Epoch:44/50 AVG Training Loss:0.012376 AVG Val Loss:6.942102 AVG Training Acc 99.20 % AVG Val Acc 54.61 %\n",
            "Epoch:45/50 AVG Training Loss:0.012752 AVG Val Loss:6.959791 AVG Training Acc 99.20 % AVG Val Acc 54.61 %\n",
            "Epoch:46/50 AVG Training Loss:0.012041 AVG Val Loss:6.968697 AVG Training Acc 99.20 % AVG Val Acc 54.61 %\n",
            "Epoch:47/50 AVG Training Loss:0.012960 AVG Val Loss:6.970194 AVG Training Acc 99.00 % AVG Val Acc 54.61 %\n",
            "Epoch:48/50 AVG Training Loss:0.012012 AVG Val Loss:6.988389 AVG Training Acc 99.00 % AVG Val Acc 54.61 %\n",
            "Epoch:49/50 AVG Training Loss:0.012517 AVG Val Loss:6.976123 AVG Training Acc 99.20 % AVG Val Acc 54.61 %\n",
            "Epoch:50/50 AVG Training Loss:0.012005 AVG Val Loss:6.983792 AVG Training Acc 99.20 % AVG Val Acc 54.61 %\n"
          ]
        }
      ],
      "source": [
        "result = model_fit(num_epochs, model, train_loader, valid_loader, optimizer, scheduler, clip)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Test Model"
      ],
      "metadata": {
        "id": "ZdXRa_DNB5FC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "27-vBkbfptyU"
      },
      "outputs": [],
      "source": [
        "test_model_1 = SentimentNet(100,128,1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "best_model_1 = torch.load('/content/best_model_sentiment_nets.pt')"
      ],
      "metadata": {
        "id": "EqUNiUd7SsPl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxNbPvcGpyEf",
        "outputId": "26c643d4-1c1f-48a0-eac4-f522734d7675"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "test_model_1.load_state_dict(best_model_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing with trained model"
      ],
      "metadata": {
        "id": "-eDbswg9CNIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "G8NDZOv9p5RQ",
        "outputId": "78f4798b-8e2f-4842-cb70-d3ab9801d308"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAHkCAYAAAD2E8+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcsklEQVR4nO3debBmd1kn8O9DAgQChMQEZQmEJUQzLBlsWaSABCwN4Ig4KgRGxxEnLAPOgJSFQw1xcHCYYRisGWVpJZVhMSKFrEZAFA0oW4ghdIDIlkADGpIOW4CE7n7mj/s23HQ6977d3Pe899zz+VS9Vfc973vOebqTTp7+/pZT3R0AgLG4ybILAAA4GJoXAGBUNC8AwKhoXgCAUdG8AACjonkBAEZF8wIADKKqzq6qK6pqx6pjr6uqi2avy6rqonWvY58XAGAIVfXQJN9I8qruvtcBPn9xkq929/PXus7hC6oPAOB6uvv8qjrhQJ9VVSX5xSQPX+86ho0AgM3gIUn+ubs/ud4XN1XyctitjuzDjzlm2WXAlnXzz1+z7BJgS/t2rsl1fW0tu471/NRpR/ZVu/Zs+HU/fPG1lyT59qpD27t7+5ynn5Hk3Hm+uKmal8OPOSZ3+I3/tOwyYMu6xzPfv+wSYEv7QP/VskuYy1W79uSD77jzhl/3sNt/8tvdve1gz6uqw5P8XJIfnef7m6p5AQAWr5Pszd5ll7HaTyT5RHfvnOfL5rwAAIOoqnOTvC/JSVW1s6qeNPvo8ZlzyCiRvADABHX29PDJS3efcSPHf+VgriN5AQBGRfICABOzMudlvJvUal4AYII22YTdg2LYCAAYFckLAExMp7NnxM82lLwAAKMieQGACTJhFwAYjU6yZ8TNi2EjAGBUJC8AMEFjHjaSvAAAoyJ5AYCJ6WTUS6U1LwAwQePdX9ewEQAwMpIXAJiYTlsqDQAwFMkLAExNJ3vGG7xIXgCAcZG8AMDEdMa92kjzAgCTU9mTWnYRh8ywEQAwKpIXAJiYTrLXhF0AgGFIXgBggsY850XzAgAT0xl382LYCAAYFckLAEzQ3pa8AAAMQvICABMz9jkvmhcAmJhOZc+IB1/GWzkAMEmSFwCYIBN2AQAGInkBgIkxYRcAGJnKnh7v4Mt4KwcAJknyAgAT00n2jji/GG/lAMAkSV4AYILGPGFX8gIAjIrkBQAmpnvcq400LwAwQXsNGwEADEPyAgATs7LD7njzi/FWDgBMkuQFACbHhF0AYETssAsAMCDJCwBM0J62VBoAYBCSFwCYmE6Neqm05gUAJmjviFcbjbdyAGCSJC8AMDF22AUAGJDkBQAmplOWSgMADEXyAgATNObHA2heAGBiujPqBzOOt3IAYJIkLwAwOZW9MWEXAGAQkhcAmJjOuOe8aF4AYILssAsAsI6qOruqrqiqHfsdf0ZVfaKqLqmq/7nedSQvADAxncre5eywe06S30/yqn0Hquq0JI9Jct/uvraqbrfeRSQvAMAguvv8JLv2O/zUJC/s7mtn37livetoXgBggvbkJhv+SnJsVV2w6nXmHKXcM8lDquoDVfW3VfVj651g2AgAJqaT7F3MaqMru3vbQZ5zeJJjkjwwyY8l+dOqult3942dIHkBAJZpZ5I/6xUfTLI3ybFrnSB5AYDJqezZPDvsvinJaUneXVX3THKzJFeudYLmBQAYRFWdm+TUrMyN2ZnkrCRnJzl7tnz6uiT/dq0ho0TzAgCTs8A5L2vft/uMG/no3xzMdcx5AQBGRfICABO0iea8HDTNCwBMTHctZdhoo4y3cgBgkiQvADBBeyQvAADDkLwAwMR0kr0m7AIA41GGjQAAhiJ5AYCJWdlhd7zDRpIXAGBUJC8AMEF7RpxfaF4AYGI6ZdgIAGAokhcAmKC9I84vxls5ADBJkhcAmJjuZI85LwAAw5C8AMAEjXm1keYFACZmZan0eAdfxls5ADBJkhcAmKA9Ge+wkeQFABgVyQsATMzYnyqteQGAyTFhFwBgMJoX5nK7cz+dE/7LBTn+f3zkBp/d9t1fzD2e+f7c5BvfWUJlsDU8639/Lq+7+JK84q8v/e6xh/z0V7L93Z/IX+z8SE68zzeXWB1b0d7Uhr+GstDmpapOr6pLq+pTVfWcRd6Lxfra/Y/Ll878kRscP/zqa3PLS7+a7xx9syVUBVvHO193TJ77xLte79hlnzgiz/+1E/LR9x+5pKpgc1pY81JVhyX5gySPTHJykjOq6uRF3Y/F+vbdb5M9Rx52g+PHvunyXPmv7ryEimBr2fGBW+XrV19/GuLnP3VEdn76iCVVxFa279lGG/0ayiIn7N4/yae6+zNJUlV/kuQxST62wHsyoCM/uiu7j7pZrrujvxUCjI0Juwd2xySfX/V+5+wYW0BdtydHv+sL2fXIOy27FAAmZulLpavqzCRnJslhRx+95GqY102vvDaH77o2x7/o4iTJ4V+9Lse/+KPZ+cx7Zc9tzH8B2MxWnm1kn5cD+UKS41e9v9Ps2PV09/Yk25Pk5nc+vhdYDxvoujvcMpf9zrbvvr/L8y/M55917+y91U2XWBUAU7DI5uVDSU6sqrtmpWl5fJInLPB+LNAPvuqTucWnvpbDrtmdE377wlx1+p3y9QfebtllwZbxnJdenvs86Bs56pjdec0FH8urX/yD+frVh+dp/+0LOeoHdud3Xv3ZfPqSI/LcJ9x92aWyRQy5tHmjLax56e7dVfX0JO9IcliSs7v7kkXdj8X6518+cc3PL3/e/QaqBLamFz7tLgc8/vdvP2rgSmDzW+icl+4+L8l5i7wHAHBwPNsIABgdS6UBAAYieQGAqelxL5WWvAAAoyJ5AYCJ6VgqDQCMjGEjAICBSF4AYGLGvs+L5AUAGBXJCwBM0JiTF80LAExMxz4vAACDkbwAwASNeZ8XyQsAMCqSFwCYmh73hF3JCwAwKpIXAJiYsW9Sp3kBgAkac/Ni2AgAGBXJCwBMjE3qAAAGJHkBgAnqEScvmhcAmCA77AIADETyAgAT03bYBQAYjuQFACZozBN2JS8AMDkr+7xs9Gvdu1adXVVXVNWOVcd+u6q+UFUXzV6PWu86mhcAYCjnJDn9AMdf0t2nzF7nrXcRw0YAMEHLGDbq7vOr6oTv9zqSFwBg2Z5eVRfPhpWOXu/LmhcAmJhOFjXn5diqumDV68w5ynlZkrsnOSXJl5K8eL0TDBsBABvlyu7edjAndPc/7/u5qv4wydvWO0fzAgBT0ysb1W0GVXX77v7S7O1jk+xY6/uJ5gUAJmkZzzaqqnOTnJqV4aWdSc5KcmpVnZKV0azLkjx5vetoXgCAQXT3GQc4/MqDvY7mBQAmpmOHXQCAwUheAGBy5tvOf7PSvADABG2W1UaHwrARADAqkhcAmCATdgEABiJ5AYCJ6R538qJ5AYAJGvNqI8NGAMCoSF4AYIIslQYAGIjkBQAmyIRdAGA0OjXq5sWwEQAwKpIXAJigEc/XlbwAAOMieQGAqRn5DruSFwBgVCQvADBFI570onkBgAkybAQAMBDJCwBMkGcbAQAMRPICABPTGfecF80LAExNJxlx82LYCAAYFckLAEyQCbsAAAORvADAFI04edG8AMDk1KhXGxk2AgBGRfICAFM04mEjyQsAMCqSFwCYmh73DruSFwBgVCQvADBFI57zonkBgEkybAQAMAjJCwBM0YiHjSQvAMCoSF4AYIpGnLxoXgBgajqJfV4AAIYheQGACeqtOGxUVf83a4yIdfevL6QiAIA1rJW8XDBYFQDAsLZi8tLd/2/1+6q6ZXd/c/ElAQALt5Un7FbVg6rqY0k+MXt/36p66cIrAwA4gHlWG/1ekp9KclWSdPdHkjx0kUUBAItVvfGvocy1VLq7P7/foT0LqAUAYF3zLJX+fFX9eJKuqpsm+Y9JPr7YsgCAhemMesLuPMnLU5L8hyR3TPLFJKfM3gMADG7d5KW7r0zyxAFqAQAGUVt+tdHdquqtVfXlqrqiqt5cVXcbojgAYEF6Aa+BzDNs9MdJ/jTJ7ZPcIcnrk5y7yKIAAG7MPM3LLbv71d29e/Z6TZIjFl0YALBAI05e1nq20TGzH/+iqp6T5E+yUtrjkpw3QG0AADew1oTdD2elWdk3o+fJqz7rJL+1qKIAgAUb8VLptZ5tdNchCwEABtIZ9WqjeTapS1XdK8nJWTXXpbtftaiiAABuzLrNS1WdleTUrDQv5yV5ZJL3JtG8AMBIDfksoo02z2qjn0/yiCT/1N3/Lsl9kxy10KoAAG7EPMNG3+ruvVW1u6puk+SKJMcvuC4AYJG2ePJyQVXdNskfZmUF0oVJ3rfQqgCALaeqzp7t1r/jAJ/9RlV1VR273nXmebbR02Y/vryq3p7kNt198cGXDABM3DlJfj/7zZutquOT/GSSz81zkbU2qbvfWp9194VzlQkAbDrLmLDb3edX1QkH+OglSX4zyZvnuc5aycuL17p/kofPc4ODce+jv5wPPu7lG31ZYOZHP/7UZZcAW9ruN7x/2SUs27FVdcGq99u7e/taJ1TVY5J8obs/UjXf3jNrbVJ32lxXAADGZzGb1F3Z3dvm/XJV3TLJf87KkNHc5pmwCwCwCHdPctckH6mqy5LcKcmFVfVDa5001w67AMAWMvBToG9Md380ye32vZ81MNu6+8q1zpO8AMAU9QJe66iqc7Oy3cpJVbWzqp50KKXP83iASvLEJHfr7udX1Z2T/FB3f/BQbggATFN3n7HO5yfMc515kpeXJnlQkn03/HqSP5jn4gDA5lS98a+hzDPn5QHdfb+q+ock6e6rq+pmC64LAOCA5mlevlNVh2U2mlVVxyXZu9CqAIDF2gQTdg/VPMNG/yfJG5PcrqpekOS9SX53oVUBAIu1hAm7G2WeZxu9tqo+nOQRSSrJz3b3xxdeGQDAAcyz2ujOSb6Z5K2rj3X3XA9PAgA2l6En2G60eea8/HlWwqBKckRWdsK7NMm/WGBdAAAHNM+w0b1Xv589bfppC6sIAFi8xTzbaBAH/XiA7r6wqh6wiGIAgIFs5WGjqnrWqrc3SXK/JF9cWEUAAGuYJ3m59aqfd2dlDswbFlMOADCELTthd7Y53a27+9kD1QMAsKYbbV6q6vDu3l1VDx6yIABgAFs0eflgVua3XFRVb0ny+iTX7Puwu/9swbUBANzAPHNejkhyVZKH53v7vXQSzQsAjNEW3qTudrOVRjvyvaZlnxH/kgGAMf+ffK3m5bAkt8r1m5Z9RvxLBgDGbK3m5Uvd/fzBKgEAhjPiGOIma3w23n2DAYAta63k5RGDVQEADGrME3ZvNHnp7l1DFgIAMI+1ho0AADadg36qNACwBWzFYSMAgM1I8gIAU7OFd9gFALaqETcvho0AgFGRvADAFEleAACGIXkBgImpjHvCruQFABgVyQsATNGIkxfNCwBMzcj3eTFsBACMiuQFAKZI8gIAMAzJCwBM0YiTF80LAEyQCbsAAAORvADAFEleAACGIXkBgKnpjDp50bwAwASZsAsAMBDJCwBMkeQFAGAYkhcAmCBzXgAABiJ5AYApGnHyonkBgKkZ+T4vho0AgFGRvADAxNTsNVaSFwBgVCQvADBFI57zonkBgAmyzwsAwEAkLwAwRZIXAIBhSF4AYIpGnLxoXgBgatqEXQCAwUheAGCKJC8AAMPQvADABFVv/Gvde1adXVVXVNWOVcd+p6ourqqLquqdVXWH9a6jeQEAhnJOktP3O/ai7r5Pd5+S5G1JnrfeRcx5AYApWsKcl+4+v6pO2O/Y11a9PTJzVKZ5AYAJ2kxLpavqBUl+OclXk5y23vcNGwEAG+XYqrpg1evMeU7q7ud29/FJXpvk6et9X/ICAFPTWdSw0ZXdve37OP+1Sc5LctZaX5K8AABLU1Unrnr7mCSfWO8cyQsATNES5rxU1blJTs3K8NLOrCQsj6qqk5LsTXJ5kqesdx3NCwBMTGU5E3a7+4wDHH7lwV7HsBEAMCqSFwCYok20VPpgSV4AgFGRvADABFWPN3rRvADA1Cxun5dBGDYCAEZF8gIAE7SZnm10sCQvAMCoSF4AYIpGnLxoXgBgggwbAQAMRPICAFMkeQEAGIbkBQCmps15AQAYjOQFAKZoxMmL5gUAJqZi2AgAYDCSFwCYoh5v9CJ5AQBGRfICABM05jkvmhcAmJrOqFcbGTYCAEZF8gIAE1R7l13BoZO8AACjInkBgCka8ZwXzQtzefEzj88H3nWb3PbY3dn+7kuTJC948l2y89NHJEmu+dphOfI2e/Kyd126zDJh1J73mHfnIfe8PLuuuUUe99LHJUmeetoH87Afvix7u3L1NbfIWW86LVd+/cglV8pWMObVRgsbNqqqs6vqiqrasah7MJyffNyuvOC1n7nesee+4vK87F2X5mXvujQPfvRX8uBHfWVJ1cHW8NaLTsozXvPo6x171d+fkse/7BfzhJf/Qt7zj3fJv3/Yh5dUHWwei5zzck6S0xd4fQZ07wdek1sfveeAn3Un57/ltjntZ68euCrYWv7h8jvkq9+6+fWOXXPtzb778y1u+p1RR/1sIp2V/3hv9GsgCxs26u7zq+qERV2fzWPHB47M0cftzh3vdt2yS4Et6WkP/0Aefd9/zDeuvVmefM7PLLscWDqrjfi+vftNR+dUqQsszEv/+gF59Et+KW+/+MQ87v5G4tkY1Rv/GsrSm5eqOrOqLqiqC7581YGHJdi89uxO/u68o/KwnzHfBRbtLz56Yh5+8mfW/yJscUtvXrp7e3dv6+5tx/3AYcsuh4N04XtunePvcW2Ou8N3ll0KbEnHH/O9vxg87KTLctmVRy+vGLaWXsBrIJZKM5f//tS75OL33Spf3XV4nvijJ+eXfuOfcvoTduVv32zICDbKC/71u7LthC/mtrf8ds571qvzindvy4NP/FzucuxX0l350ldund9920OWXSZbQGXcS6UX1rxU1blJTk1ybFXtTHJWd79yUfdjsX7rZZcf8Pizf+9zA1cCW9dz3/ATNzj25n/4kSVUApvbIlcbnbGoawMA34eBlzZvtKXPeQEAOBjmvADABJnzAgCMy4ibF8NGAMCoSF4AYILGPGwkeQEARkXyAgBT00n2jjd60bwAwBSNt3cxbAQAjIvkBQAmyIRdAICBSF4AYIo82wgAYBiSFwCYoDHPedG8AMDUdCyVBgAYiuQFACamkpQJuwAAw5C8AMAU7V12AYdO8wIAE2TYCABgIJIXAJgaS6UBAIYjeQGAyelRP9tI8wIAEzTmxwMYNgIARkXyAgBTNOJhI8kLADAqmhcAmJpOau/Gv9ZTVWdX1RVVtWPVsRdV1Seq6uKqemNV3Xa962heAIChnJPk9P2O/WWSe3X3fZL8Y5LfWu8imhcAmKLujX+te8s+P8mu/Y69s7t3z96+P8md1ruOCbsAMEWLma97bFVdsOr99u7efhDn/2qS1633Jc0LALBRruzubYdyYlU9N8nuJK9d77uaFwCYoM30VOmq+pUkP53kEd3rF6Z5AQCWpqpOT/KbSR7W3d+c5xzNCwBM0RKSl6o6N8mpWZkbszPJWVlZXXTzJH9ZVUny/u5+ylrX0bwAwNR0kjn2Zdnw23afcYDDrzzY61gqDQCMiuQFACam0ptqwu7BkrwAAKMieQGAKRpx8qJ5AYApGnHzYtgIABgVyQsATM2SlkpvFMkLADAqkhcAmCBLpQEABiJ5AYApGnHyonkBgMnpUTcvho0AgFGRvADA1HQkLwAAQ5G8AMAUjXiTOs0LAEyQfV4AAAYieQGAKZK8AAAMQ/ICAFPTSfaON3nRvADA5NhhFwBgMJIXAJgiyQsAwDAkLwAwRZIXAIBhSF4AYGoslQYAxqWTHu+TGQ0bAQCjInkBgCkyYRcAYBiSFwCYGhN2AYDRMWwEADAMyQsATJHkBQBgGJIXAJicHnXyonkBgKnpJHvtsAsAMAjJCwBM0YiHjSQvAMCoSF4AYIokLwAAw5C8AMDktGcbAQAj0km3pdIAAIOQvADAFI142EjyAgCMiuQFAKZoxEulNS8AMDXdnm0EADAUyQsATNGIh40kLwDAqEheAGCCesRzXjQvADA5bdgIAGAokhcAmJqOHXYBAIYieQGAKfJUaQCAYUheAGBiOkmPeM6L5gUApqbbsBEAwHqq6uyquqKqdqw69gtVdUlV7a2qbfNcR/MCABPUe3vDX3M4J8np+x3bkeTnkpw/b+2GjQCAQXT3+VV1wn7HPp4kVTX3dTQvADBFI57zUr2Jnm1QVV9Ocvmy62Buxya5ctlFwBbnz9m43KW7j1t2Eeupqrdn5d+tjXZEkm+ver+9u7fvd+8Tkrytu++13/G/SfLs7r5gvZtsquRlDP/A+Z6quqC755pcBRwaf85YhO7ef97JqJiwCwCMiuYFABhEVZ2b5H1JTqqqnVX1pKp6bFXtTPKgJH9eVe9Y7zqbatiI0dm+/leA75M/Z2wZ3X3GjXz0xoO5zqaasAsAsB7DRgDAqGheOCRVdXpVXVpVn6qq5yy7HthqDrSNOrBC88JBq6rDkvxBkkcmOTnJGVV18nKrgi3nnNxwG3UgmhcOzf2TfKq7P9Pd1yX5kySPWXJNsKV09/lJdi27DtiMNC8cijsm+fyq9ztnxwBg4TQvAMCoaF44FF9Icvyq93eaHQOAhdO8cCg+lOTEqrprVd0syeOTvGXJNQEwEZoXDlp3707y9CTvSPLxJH/a3ZcstyrYWg60jfqya4LNwg67AMCoSF4AgFHRvAAAo6J5AQBGRfMCAIyK5gUAGBXNCyxQVe2pqouqakdVvb6qbvl9XOucqvr52c9/tNbDMKvq1Kr68UO4x2VVdey8x/f7zjcO8l6/XVXPPtgaATQvsFjf6u5TuvteSa5L8pTVH1bV4Ydy0e7+te7+2BpfOTXJQTcvAGOgeYHhvCfJPWapyHuq6i1JPlZVh1XVi6rqQ1V1cVU9OUlqxe9X1aVV9a4kt9t3oar6m6raNvv59Kq6sKo+UlV/VVUnZKVJeuYs9XlIVR1XVW+Y3eNDVfXg2bk/UFXvrKpLquqPktR6v4iqelNVfXh2zpn7ffaS2fG/qqrjZsfuXlVvn53znqr64Y34zQSm65D+1gccnFnC8sgkb58dul+Se3X3Z2cNwFe7+8eq6uZJ/q6q3pnkXyY5KcnJSX4wyceSnL3fdY9L8odJHjq71jHdvauqXp7kG939v2bf++MkL+nu91bVnbOyO/KPJDkryXu7+/lV9egk8+zi+quze9wiyYeq6g3dfVWSI5Nc0N3PrKrnza799CTbkzyluz9ZVQ9I8tIkDz+E30aAJJoXWLRbVNVFs5/fk+SVWRnO+WB3f3Z2/CeT3GfffJYkRyU5MclDk5zb3XuSfLGq/voA139gkvP3Xau7d91IHT+R5OSq7wYrt6mqW83u8XOzc/+8qq6e49f061X12NnPx89qvSrJ3iSvmx1/TZI/m93jx5O8ftW9bz7HPQBulOYFFutb3X3K6gOz/4lfs/pQkmd09zv2+96jNrCOmyR5YHd/+wC1zK2qTs1KI/Sg7v5mVf1NkiNu5Os9u+9X9v89APh+mPMCy/eOJE+tqpsmSVXds6qOTHJ+ksfN5sTcPslpBzj3/UkeWlV3nZ17zOz415PcetX33pnkGfveVNW+ZuL8JE+YHXtkkqPXqfWoJFfPGpcfzkrys89NkuxLj56QleGoryX5bFX9wuweVVX3XeceAGvSvMDy/VFW5rNcWFU7krwiK6noG5N8cvbZq7LyhOHr6e4vJzkzK0M0H8n3hm3emuSx+ybsJvn1JNtmE4I/lu+tevqvWWl+LsnK8NHn1qn17UkOr6qPJ3lhVpqnfa5Jcv/Zr+HhSZ4/O/7EJE+a1XdJksfM8XsCcKM8VRoAGBXJCwAwKpoXAGBUNC8AwKhoXgCAUdG8AACjonkBAEZF8wIAjIrmBQAYlf8PIoei9P+IJNQAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "actual = []\n",
        "predicted = []\n",
        "count_batch = 0\n",
        "\n",
        "for inputs,target in test_loader:\n",
        "  inputs, target = inputs.to(device), target.to(device)\n",
        "  output = test_model_1(inputs)\n",
        "  pred = torch.round(output['network_output'])\n",
        "  count_batch += len(target)\n",
        "  actual[count_batch-len(target):count_batch] = target\n",
        "  predicted[count_batch-len(target):count_batch] = pred\n",
        "  \n",
        "  prediction_int = np.array([x.item() for x in predicted])\n",
        "  actual_int = np.array([x.item() for x in actual])\n",
        "\n",
        "confusion = confusion_matrix(prediction_int,actual_int)\n",
        "matrix = ConfusionMatrixDisplay(confusion_matrix=confusion)\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "matrix.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8BittqKqEkV",
        "outputId": "2c1ee823-ac4a-4246-9832-8574585b8bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.45      0.56      0.50        25\n",
            "         1.0       0.54      0.43      0.48        30\n",
            "\n",
            "    accuracy                           0.49        55\n",
            "   macro avg       0.50      0.50      0.49        55\n",
            "weighted avg       0.50      0.49      0.49        55\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_true = actual_int\n",
        "y_pred = prediction_int\n",
        "print(classification_report(y_pred, y_true))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing with best model"
      ],
      "metadata": {
        "id": "11Awf1wxCQD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_test = SentimentNet(100,128,1).to(device)"
      ],
      "metadata": {
        "id": "G59ySOHXCRug"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_test.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW4OtcCdCl3e",
        "outputId": "964065c1-fa89-444e-caec-ae9d79911fcb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual = []\n",
        "predicted = []\n",
        "count_batch = 0\n",
        "\n",
        "for inputs,target in test_loader:\n",
        "  inputs, target = inputs.to(device), target.to(device)\n",
        "  output = best_model_test(inputs)\n",
        "  pred = torch.round(output['network_output'])\n",
        "  count_batch += len(target)\n",
        "  actual[count_batch-len(target):count_batch] = target\n",
        "  predicted[count_batch-len(target):count_batch] = pred\n",
        "  \n",
        "  prediction_int = np.array([x.item() for x in predicted])\n",
        "  actual_int = np.array([x.item() for x in actual])\n",
        "\n",
        "confusion = confusion_matrix(prediction_int,actual_int)\n",
        "matrix = ConfusionMatrixDisplay(confusion_matrix=confusion)\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "matrix.plot(ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "Js4FJVdUCpj5",
        "outputId": "e2aadb64-c012-4d71-d7e3-ce731dd1ac9d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = actual_int\n",
        "y_pred = prediction_int\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJOvQCgKC4Tz",
        "outputId": "f90ee8b6-4368-422a-84d9-6fcd0a4d02a9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.73      0.75        33\n",
            "         1.0       0.62      0.68      0.65        22\n",
            "\n",
            "    accuracy                           0.71        55\n",
            "   macro avg       0.70      0.70      0.70        55\n",
            "weighted avg       0.71      0.71      0.71        55\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best model resulted 71% of accuracy in testing mode."
      ],
      "metadata": {
        "id": "kfOhBpr3DKbE"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sentiment Analysis_LSTM-Glove.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}